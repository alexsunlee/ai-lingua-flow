import 'dart:io';

import 'package:youtube_explode_dart/youtube_explode_dart.dart';

import '../../../../core/network/gemini_client.dart';
import '../../../../core/storage/file_storage_service.dart';

/// Raw caption segment returned by the datasource.
class RawCaptionSegment {
  final int startMs;
  final int endMs;
  final String text;

  const RawCaptionSegment({
    required this.startMs,
    required this.endMs,
    required this.text,
  });
}

/// Metadata fetched from YouTube.
class VideoMetadata {
  final String title;
  final String? channelName;
  final String? thumbnailUrl;
  final int? durationSeconds;

  const VideoMetadata({
    required this.title,
    this.channelName,
    this.thumbnailUrl,
    this.durationSeconds,
  });
}

/// Datasource that uses youtube_explode_dart to fetch video metadata,
/// extract captions with timestamps, and download audio.
/// Falls back to Gemini for transcription when captions are unavailable.
class YouTubeDatasource {
  final GeminiClient _geminiClient;
  final FileStorageService _fileStorageService;

  YouTubeDatasource({
    required GeminiClient geminiClient,
    required FileStorageService fileStorageService,
  })  : _geminiClient = geminiClient,
        _fileStorageService = fileStorageService;

  /// Fetch video metadata (title, channel, thumbnail, duration).
  Future<VideoMetadata> fetchVideoMetadata(String videoId) async {
    final yt = YoutubeExplode();
    try {
      final video = await yt.videos.get(videoId);
      return VideoMetadata(
        title: video.title,
        channelName: video.author,
        thumbnailUrl: video.thumbnails.highResUrl,
        durationSeconds: video.duration?.inSeconds,
      );
    } finally {
      yt.close();
    }
  }

  /// Extract captions with timestamps from the video.
  /// Tries closed captions first; if unavailable, returns an empty list.
  /// The caller can then attempt Gemini-based transcription as a fallback.
  Future<List<RawCaptionSegment>> extractCaptions(String videoId) async {
    final yt = YoutubeExplode();
    try {
      final manifest = await yt.videos.closedCaptions.getManifest(videoId);

      // Prefer English captions, then auto-generated English, then any.
      ClosedCaptionTrackInfo? trackInfo;
      trackInfo = manifest.tracks.where((t) {
        return t.language.code == 'en' && !t.isAutoGenerated;
      }).firstOrNull;

      trackInfo ??= manifest.tracks.where((t) {
        return t.language.code == 'en';
      }).firstOrNull;

      trackInfo ??= manifest.tracks.firstOrNull;

      if (trackInfo == null) {
        return [];
      }

      final track = await yt.videos.closedCaptions.get(trackInfo);

      final segments = <RawCaptionSegment>[];
      for (final caption in track.captions) {
        final startMs = caption.offset.inMilliseconds;
        final endMs = startMs + caption.duration.inMilliseconds;
        final text = caption.text.trim();
        if (text.isNotEmpty) {
          segments.add(RawCaptionSegment(
            startMs: startMs,
            endMs: endMs,
            text: text,
          ));
        }
      }
      return segments;
    } finally {
      yt.close();
    }
  }

  /// Download the audio stream of a video and save it locally.
  /// Returns the file path of the saved audio file.
  Future<String> downloadAudio(String videoId) async {
    final yt = YoutubeExplode();
    try {
      final manifest = await yt.videos.streamsClient.getManifest(videoId);
      final audioStreamInfo = manifest.audioOnly.withHighestBitrate();

      final audioStream = yt.videos.streamsClient.get(audioStreamInfo);

      final filePath =
          _fileStorageService.audioFilePath('$videoId.${audioStreamInfo.container.name}');
      final file = File(filePath);
      final fileStream = file.openWrite();

      await for (final chunk in audioStream) {
        fileStream.add(chunk);
      }
      await fileStream.flush();
      await fileStream.close();

      return filePath;
    } finally {
      yt.close();
    }
  }

  /// Gemini fallback for transcription when captions are unavailable.
  /// Takes the full transcript text (e.g. from audio recognition) and
  /// asks Gemini to segment it with approximate timestamps.
  Future<List<RawCaptionSegment>> transcribeWithGemini({
    required String rawText,
    required int durationSeconds,
  }) async {
    final prompt = '''
You are a transcription assistant. Given the following raw text from a video
that is $durationSeconds seconds long, split it into logical segments of
1-3 sentences each. For each segment, estimate the start and end timestamps
in milliseconds based on the text position relative to total duration.

Return a JSON array where each element has:
- "start_ms": integer, estimated start time in milliseconds
- "end_ms": integer, estimated end time in milliseconds
- "text": string, the segment text

Raw text:
$rawText
''';

    final result = await _geminiClient.generateStructured(prompt: prompt);
    final segmentsList = result['segments'] as List? ?? [];

    return segmentsList.map((s) {
      final map = s as Map<String, dynamic>;
      return RawCaptionSegment(
        startMs: (map['start_ms'] as num).toInt(),
        endMs: (map['end_ms'] as num).toInt(),
        text: (map['text'] as String).trim(),
      );
    }).toList();
  }
}
