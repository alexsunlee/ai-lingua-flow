import 'dart:async';
import 'dart:io';

import 'package:flutter/foundation.dart';
import 'package:youtube_explode_dart/youtube_explode_dart.dart';

import '../../../../core/network/gemini_client.dart';
import '../../../../core/storage/file_storage_service.dart';

/// Raw caption segment returned by the datasource.
class RawCaptionSegment {
  final int startMs;
  final int endMs;
  final String text;
  final String? translation;

  const RawCaptionSegment({
    required this.startMs,
    required this.endMs,
    required this.text,
    this.translation,
  });
}

/// Metadata fetched from YouTube.
class VideoMetadata {
  final String title;
  final String? channelName;
  final String? thumbnailUrl;
  final int? durationSeconds;

  const VideoMetadata({
    required this.title,
    this.channelName,
    this.thumbnailUrl,
    this.durationSeconds,
  });
}

/// Datasource that uses youtube_explode_dart to fetch video metadata,
/// extract captions with timestamps, and download audio.
/// Falls back to Gemini for transcription when captions are unavailable.
class YouTubeDatasource {
  final GeminiClient _geminiClient;
  final FileStorageService _fileStorageService;

  YouTubeDatasource({
    required GeminiClient geminiClient,
    required FileStorageService fileStorageService,
  })  : _geminiClient = geminiClient,
        _fileStorageService = fileStorageService;

  /// Fetch video metadata (title, channel, thumbnail, duration).
  Future<VideoMetadata> fetchVideoMetadata(String videoId) async {
    final yt = YoutubeExplode();
    try {
      final video = await yt.videos.get(videoId);
      return VideoMetadata(
        title: video.title,
        channelName: video.author,
        thumbnailUrl: video.thumbnails.highResUrl,
        durationSeconds: video.duration?.inSeconds,
      );
    } finally {
      yt.close();
    }
  }

  /// Extract captions with timestamps from the video.
  /// Tries closed captions first; if unavailable, returns an empty list.
  /// The caller can then attempt Gemini-based transcription as a fallback.
  Future<List<RawCaptionSegment>> extractCaptions(String videoId) async {
    final yt = YoutubeExplode();
    try {
      final manifest = await yt.videos.closedCaptions.getManifest(videoId);

      // Prefer English captions, then auto-generated English, then any.
      ClosedCaptionTrackInfo? trackInfo;
      trackInfo = manifest.tracks.where((t) {
        return t.language.code == 'en' && !t.isAutoGenerated;
      }).firstOrNull;

      trackInfo ??= manifest.tracks.where((t) {
        return t.language.code == 'en';
      }).firstOrNull;

      trackInfo ??= manifest.tracks.firstOrNull;

      if (trackInfo == null) {
        return [];
      }

      final track = await yt.videos.closedCaptions.get(trackInfo);

      final segments = <RawCaptionSegment>[];
      for (final caption in track.captions) {
        final startMs = caption.offset.inMilliseconds;
        final endMs = startMs + caption.duration.inMilliseconds;
        final text = caption.text.trim();
        if (text.isNotEmpty) {
          segments.add(RawCaptionSegment(
            startMs: startMs,
            endMs: endMs,
            text: text,
          ));
        }
      }
      return segments;
    } catch (e) {
      // Some videos have captions listed but malformed XML data.
      debugPrint('Caption extraction failed for $videoId: $e');
      return [];
    } finally {
      yt.close();
    }
  }

  /// Get a playable video stream URL for the given video ID.
  /// Returns the URL of the best available muxed stream (video+audio).
  Future<String> getVideoStreamUrl(String videoId) async {
    final yt = YoutubeExplode();
    try {
      final manifest = await yt.videos.streamsClient.getManifest(videoId);
      // Prefer muxed streams (video + audio combined) for direct playback
      final muxed = manifest.muxed.sortByVideoQuality();
      if (muxed.isNotEmpty) {
        return muxed.first.url.toString();
      }
      // Fallback to audio-only if no muxed available
      final audio = manifest.audioOnly.withHighestBitrate();
      return audio.url.toString();
    } finally {
      yt.close();
    }
  }

  /// Download the audio stream of a video and save it locally.
  /// Returns the file path of the saved audio file.
  /// Uses lowest bitrate (sufficient for speech transcription) and
  /// times out after 180 seconds to handle slow connections.
  Future<String> downloadAudio(String videoId) async {
    final yt = YoutubeExplode();
    String? filePath;
    try {
      debugPrint('[downloadAudio] Fetching stream manifest for $videoId...');
      final manifest = await yt.videos.streamsClient.getManifest(videoId);

      // Use lowest bitrate — we only need speech, not music quality.
      final audioStreams = manifest.audioOnly.sortByBitrate();
      final audioStreamInfo = audioStreams.first;
      debugPrint('[downloadAudio] Got manifest — '
          'container=${audioStreamInfo.container.name}, '
          'bitrate=${audioStreamInfo.bitrate.kiloBitsPerSecond}kbps, '
          'size=${audioStreamInfo.size.totalBytes} bytes');

      final audioStream = yt.videos.streamsClient.get(audioStreamInfo);

      filePath = _fileStorageService
          .audioFilePath('$videoId.${audioStreamInfo.container.name}');
      final file = File(filePath);
      final fileStream = file.openWrite();

      debugPrint('[downloadAudio] Starting download to $filePath...');
      await (() async {
        await for (final chunk in audioStream) {
          fileStream.add(chunk);
        }
        await fileStream.flush();
        await fileStream.close();
      })()
          .timeout(const Duration(seconds: 180));

      debugPrint('[downloadAudio] Download complete: $filePath '
          '(${await file.length()} bytes)');
      return filePath;
    } on TimeoutException {
      debugPrint('[downloadAudio] Timed out after 180s for $videoId');
      if (filePath != null) {
        final partial = File(filePath);
        if (await partial.exists()) await partial.delete();
      }
      throw Exception('Audio download timed out after 180 seconds');
    } finally {
      yt.close();
    }
  }

  /// Transcribe a YouTube video using Gemini's native video understanding.
  /// Sends the YouTube URL directly to Gemini — no download needed.
  /// Returns English text + timestamps only; translation is done separately.
  ///
  /// If [isRetry] is true, uses a more explicit prompt to force non-zero
  /// timestamps (called automatically when the first attempt returns all 0s).
  Future<List<RawCaptionSegment>> transcribeVideoWithGemini({
    required String youtubeUrl,
    bool isRetry = false,
  }) async {
    final prompt = isRetry
        ? '''
Watch this YouTube video from beginning to end. The video has audio speech.
Transcribe ONLY the spoken words you hear, with the EXACT time each segment is spoken.

Use the video timeline to determine timestamps. For example, if someone starts
speaking at the 15-second mark, that segment's start_ms should be 15000.

IMPORTANT RULES:
- start_ms and end_ms must NOT all be 0. Use the actual video timeline.
- The first segment's start_ms should match when speech actually begins in the video.
- Segments must be in chronological order with increasing timestamps.
- Each segment should be 5-15 seconds long.

Return a JSON object: {"segments": [{"start_ms": <int>, "end_ms": <int>, "text": "<spoken words>"}]}
'''
        : '''
Watch this YouTube video and transcribe all spoken English with precise timestamps from the video timeline.

For each segment of 1-3 sentences, record:
- The exact moment (in milliseconds) when the speech starts in the video
- The exact moment when the speech ends
- The transcribed text

Return a JSON object with a "segments" array. Each element:
- "start_ms": integer, milliseconds from video start when speech begins (use actual video time, NOT 0 for all)
- "end_ms": integer, milliseconds from video start when speech ends (must be > start_ms)
- "text": string, the spoken English text

Example for a segment starting at 1 minute 30 seconds: {"start_ms": 90000, "end_ms": 95000, "text": "Hello everyone"}

Timestamps must be monotonically increasing and reflect the real video timeline.
''';

    try {
      debugPrint('[Gemini transcribe] Sending video URL to Gemini: $youtubeUrl'
          '${isRetry ? " (retry)" : ""}');
      final result = await _geminiClient.generateStructuredWithVideo(
        prompt: prompt,
        videoUri: youtubeUrl,
      );
      debugPrint('[Gemini transcribe] Raw result keys: ${result.keys.toList()}');

      final segmentsList = result['segments'] as List? ?? [];
      debugPrint('[Gemini transcribe] Segments count from JSON: ${segmentsList.length}');

      final segments = <RawCaptionSegment>[];
      for (int i = 0; i < segmentsList.length; i++) {
        final map = segmentsList[i] as Map<String, dynamic>;
        final startMs = (map['start_ms'] as num?)?.toInt();
        final endMs = (map['end_ms'] as num?)?.toInt();
        final text = (map['text'] as String?)?.trim() ?? '';
        if (text.isEmpty || startMs == null || endMs == null) {
          debugPrint('[Gemini transcribe] Skipped segment $i: '
              'start=$startMs end=$endMs text="${text.isEmpty ? "<empty>" : text}"');
          continue;
        }
        segments.add(RawCaptionSegment(
          startMs: startMs,
          endMs: endMs,
          text: text,
          translation: (map['translation'] as String?)?.trim(),
        ));
      }
      debugPrint('[Gemini transcribe] Valid segments after filtering: ${segments.length}');
      return segments;
    } catch (e) {
      debugPrint('[Gemini transcribe] Failed: $e');
      rethrow;
    }
  }

  /// Returns true if all segments have start_ms == 0 and end_ms == 0,
  /// which indicates the model failed to generate real timestamps.
  bool hasAllZeroTimestamps(List<RawCaptionSegment> segments) {
    if (segments.isEmpty) return false;
    return segments.every((s) => s.startMs == 0 && s.endMs == 0);
  }

  /// Transcribe audio using Gemini's audio understanding capability.
  /// Sends the audio file inline and gets back timestamped segments.
  Future<List<RawCaptionSegment>> transcribeAudioWithGemini({
    required String audioFilePath,
  }) async {
    final file = File(audioFilePath);
    final bytes = await file.readAsBytes();

    // 20MB inline limit
    if (bytes.length > 20 * 1024 * 1024) {
      throw Exception('Audio file exceeds 20MB inline limit');
    }

    // Determine MIME type from extension
    final ext = audioFilePath.split('.').last.toLowerCase();
    final mimeType = switch (ext) {
      'mp3' => 'audio/mpeg',
      'mp4' || 'm4a' => 'audio/mp4',
      'webm' => 'audio/webm',
      'ogg' => 'audio/ogg',
      'wav' => 'audio/wav',
      'flac' => 'audio/flac',
      _ => 'audio/mpeg',
    };

    const prompt = '''
You are a precise audio transcription engine.
Listen to this audio carefully and transcribe the spoken English into segments.

CRITICAL: You MUST provide accurate, non-zero timestamps based on when words are actually spoken.
- The first segment should start at the actual moment speech begins (NOT at 0 unless speech truly starts immediately).
- Each subsequent segment must have a start_ms GREATER than the previous segment's start_ms.
- end_ms must always be greater than start_ms for the same segment.
- Timestamps must reflect the real timing of speech in the audio.

Return a JSON object with a "segments" array where each element has:
- "start_ms": integer, start time in milliseconds when this segment's speech begins
- "end_ms": integer, end time in milliseconds when this segment's speech ends
- "text": string, the transcribed English text for that segment

Split into segments of 1-3 sentences each. Ensure timestamps are monotonically increasing and match the actual audio timing.
''';

    try {
      debugPrint('[Gemini audio] Sending ${bytes.length} bytes ($mimeType) to Gemini...');
      final result = await _geminiClient.generateStructuredWithAudio(
        prompt: prompt,
        audioBytes: bytes,
        mimeType: mimeType,
      );
      debugPrint('[Gemini audio] Raw result keys: ${result.keys.toList()}');

      final segmentsList = result['segments'] as List? ?? [];
      debugPrint('[Gemini audio] Segments count from JSON: ${segmentsList.length}');

      final segments = <RawCaptionSegment>[];
      for (int i = 0; i < segmentsList.length; i++) {
        final map = segmentsList[i] as Map<String, dynamic>;
        final startMs = (map['start_ms'] as num?)?.toInt();
        final endMs = (map['end_ms'] as num?)?.toInt();
        final text = (map['text'] as String?)?.trim() ?? '';
        if (text.isEmpty || startMs == null || endMs == null) {
          debugPrint('[Gemini audio] Skipped segment $i: '
              'start=$startMs end=$endMs text="${text.isEmpty ? "<empty>" : text}"');
          continue;
        }
        segments.add(RawCaptionSegment(
          startMs: startMs,
          endMs: endMs,
          text: text,
        ));
      }
      debugPrint('[Gemini audio] Valid segments after filtering: ${segments.length}');
      return segments;
    } catch (e) {
      debugPrint('[Gemini audio] Transcription failed: $e');
      rethrow;
    }
  }

  /// Gemini fallback for transcription when captions are unavailable.
  /// Takes the full transcript text (e.g. from audio recognition) and
  /// asks Gemini to segment it with approximate timestamps.
  Future<List<RawCaptionSegment>> transcribeWithGemini({
    required String rawText,
    required int durationSeconds,
  }) async {
    final prompt = '''
You are a transcription assistant. Given the following raw text from a video
that is $durationSeconds seconds long, split it into logical segments of
1-3 sentences each. For each segment, estimate the start and end timestamps
in milliseconds based on the text position relative to total duration.

Return a JSON array where each element has:
- "start_ms": integer, estimated start time in milliseconds
- "end_ms": integer, estimated end time in milliseconds
- "text": string, the segment text

Raw text:
$rawText
''';

    final result = await _geminiClient.generateStructured(prompt: prompt);
    final segmentsList = result['segments'] as List? ?? [];

    return segmentsList.map((s) {
      final map = s as Map<String, dynamic>;
      return RawCaptionSegment(
        startMs: (map['start_ms'] as num).toInt(),
        endMs: (map['end_ms'] as num).toInt(),
        text: (map['text'] as String).trim(),
      );
    }).toList();
  }

  /// Translate already-generated English segments into Chinese using text generation.
  /// Returns new segments with translations filled in.
  Future<List<RawCaptionSegment>> translateSegments(
    List<RawCaptionSegment> segments,
  ) async {
    if (segments.isEmpty) return segments;

    // Build a numbered list of texts for batch translation.
    final buffer = StringBuffer();
    for (int i = 0; i < segments.length; i++) {
      buffer.writeln('$i: ${segments[i].text}');
    }

    final prompt = '''
Translate each numbered English sentence below into Chinese.
Return a JSON object with a "translations" array where each element has:
- "index": integer, the sentence number
- "translation": string, the Chinese translation

Sentences:
$buffer''';

    try {
      debugPrint('[translateSegments] Translating ${segments.length} segments...');
      final result = await _geminiClient.generateStructured(prompt: prompt);
      final translations = result['translations'] as List? ?? [];

      // Build index → translation map.
      final translationMap = <int, String>{};
      for (final t in translations) {
        final map = t as Map<String, dynamic>;
        final index = (map['index'] as num?)?.toInt();
        final text = (map['translation'] as String?)?.trim();
        if (index != null && text != null && text.isNotEmpty) {
          translationMap[index] = text;
        }
      }

      debugPrint('[translateSegments] Got ${translationMap.length} translations');
      return [
        for (int i = 0; i < segments.length; i++)
          RawCaptionSegment(
            startMs: segments[i].startMs,
            endMs: segments[i].endMs,
            text: segments[i].text,
            translation: translationMap[i] ?? segments[i].translation,
          ),
      ];
    } catch (e) {
      debugPrint('[translateSegments] Translation failed: $e');
      // Return original segments without translation rather than failing.
      return segments;
    }
  }
}
